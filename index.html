<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RiEMann: Near Real-Time SE(3)-Equivariant Robot
        Manipulation without Point Cloud Segmentation.">
  <meta name="keywords" content="Equivariant Learning, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/build/three.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/loaders/OBJLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/loaders/PLYLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/controls/OrbitControls.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chongkaigao.com/">Chongkai Gao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://steven-xzr.github.io/">Zhengrong Xue</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/ShuyingDeng">Shuying Deng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://tinhal.github.io/">Tianhai Liang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://riemann-web.github.io/">Siqi Yang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://linsats.github.io/">Lin Shao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://hxu.rocks/">Huazhe Xu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Unversity of Singapore,</span>
            <span class="author-block"><sup>2</sup>Tsinghua Unversity</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.19460"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/HeegerGao/RiEMann"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 20px;">
        RiEMann generalizes to new SE(3) poses, new object instances, and resists visual distracting, and has real-time following ability,
        only with 5 to 10 demonstrations for object manipulation tasks.
      </h2>
    </div>
  </div>
</section>

<br>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from 
            scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the 
            target poses of objects for manipulation without any object segmentation. 
          </p>
          <p>
            RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations 
            and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose 
            change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions 
            such as the direction of turning the faucet, which makes artic- ulated object manipulation possible for RiEMann. 
          </p>
          <p>
            In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks 
            with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic 
            distance errors on predicted poses (reduced by 68.6%), and achieves a 5.4 frames per second (fps) network inference speed.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<br>
<br>

<section class="section" style="padding: 0">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video</h2>
      </div>
    </div>
      <div class="columns is-centered has-text-centered">
          <video poster="" id="" autoplay="" controls="" loop="" width="100%" playbackrate="1.0" style="border-radius: 5px;">
              <source src="./static/videos/RiEMann_web.mp4" type="video/mp4">
          </video>
      </div>
  </div>
</section>

<br>
<br>

<section class="section">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">RiEMann Pipeline</span></h2>
                  <img src="./static/images/network.jpg" class="interpolation-image" alt=""
                      style="display: block; margin-left: auto; margin-right: auto; max-width: 100%;" />
                  <br>
                  <span style="font-size: 125%">
                    <p>
                      RiEMann models the SE(3)-equivariant action space of robot manipulation tasks as target poses, consisting of 
                      a <b>translational vector \( \mathbf{t} \in \mathbb{R}^3 \)</b> and a <b>rotation matrix \( \mathbf{R} \in \mathbb{R}^9 \)</b>, 
                      which are proven to be SE(3)-equivariant.
                    </p>
                    <p>
                    For a point cloud input of a scene, a type-0 saliency map is firstly outputted by an SE(3)-invariant
                    backbone  \( φ \) to get a small point cloud region \( B_{ROI} \), and an SE(3)-equivariant policy network 
                    that contains a translational heatmap network \( ψ_1 \) and an orientation network \( ψ_2 \) predicts the action vector 
                    fields on the points of \( B_{ROI} \) . Finally, we perform softmax, mean pooling, and Iterative Modified Gram-Schmidt 
                    orthogonalization to get the target action \( T \).
                    </p>
                  </span>
              </div>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">Zero-shot Generalization</span></h2>
                  <span style="font-size: 125%">
                    <p>
                      We present the video, point cloud with predicted pose, position heatmap, and orientation vector fields of 
                      mug and plane experiments to show the generalization ability of RiEMann to unseen SE(3) transformations, 
                      new object instances, distracting objects, and the combination of all of them.
                    </p>
                  <br>
                  <br>
                  <br>
                  </span>
                  <video id="mug-vis" autoplay muted loop playsinline height="100%">
                    <source src="./static/videos/mug_vis.mp4"
                            type="video/mp4">
                  </video>
                  <br>
                  <video id="mug-vis" autoplay muted loop playsinline height="100%">
                    <source src="./static/videos/plane_vis.mp4"
                            type="video/mp4">
                  </video>
              </div>
          </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">Near Real-time Following</span></h2>
                  <span style="font-size: 125%">
                    <p>
                      Thanks to the end-to-end pipleine of RiEMann, the network forward speed can be 5.4 FPS, which leads to
                      the near real-time following experiments as follows.
                    </p>
                  <br>
                  </span>
                  <video id="real-time" autoplay muted loop playsinline height="100%">
                    <source src="./static/videos/real_time.mp4"
                            type="video/mp4">
                  </video>
              </div>
          </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">More Than Target Pose</span></h2>
                  <span style="font-size: 125%">
                    <p>
                      Besides the target pose, RiEMann can also predict any type-l vector fields that are SE(3)-equivariant, as long as
                       given demonstrations.
                      Here we show an example of predicting the direction of turning the faucet.
                    </p>
                  <br>
                  <br>
                  <br>
                  </span>
                  <video id="real-time" autoplay muted loop playsinline height="60%">
                    <source src="./static/videos/faucet_with_arrow.mp4"
                            type="video/mp4">
                  </video>
              </div>
          </div>
      </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gao2024riemann,
  author    = {Gao, Chongkai and Xue, Zhengrong and Deng, Shuying and Liang, Tianhai and Yang, Siqi and Shao, Lin and Xu, Huazhe},
  title     = {RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation},
  booktitle = {arXiv preprint arXiv:2403.19460},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built upon the
             <a href="https://nerfies.github.io/">Nerfies templete</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
